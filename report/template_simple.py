from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Union
import pandas as pd
import os
from pathlib import Path
import logging
import glob
import requests

# ì‹¬í”Œ í…œí”Œë¦¿ (ìŠ¬ëž™ìš© ë§ˆí¬ë‹¤ìš´)
class SimpleTemplate:
    def __init__(self, report_instance):
        self.report = report_instance
        self.config = report_instance.config
        self.logger = logging.getLogger(__name__)

    async def create_report(self, target: str, time_range: str, output_dir: str = None, request_id: str = None) -> Dict[str, str]:
        try:
            # Calculate time range
            end_time = datetime.now()
            if time_range.endswith('h'):
                start_time = end_time - timedelta(hours=int(time_range[:-1]))
            elif time_range.endswith('d'):
                start_time = end_time - timedelta(days=int(time_range[:-1]))
            else:
                start_time = end_time.replace(hour=0, minute=0, second=0, microsecond=0)

            # Get server info
            server_info = self._get_server_info(target)
            
            # Get metrics
            metrics = await self._get_metrics(target, start_time, end_time)
            
            # Generate report sections
            header = self._generate_header(server_info)
            basic_info = self._generate_basic_info(server_info)
            metrics_info = self._generate_metrics_info(metrics)
            # analysis = await self._generate_analysis(server_info, metrics)

            # ê¸°ë³¸ ë©”íŠ¸ë¦­ ì¦‰ì‹œ ë°˜í™˜
            report = f"{header}\n\n{basic_info}\n\n{metrics_info}"
            # ëŠë ¤í„°ì§„ LLMì€ ìŠ¤ë ˆë“œ ì²˜ë¦¬ í›„ ë°˜í™˜
            analysis = await self._generate_analysis(server_info, metrics)

            return {
                "report": report,
                "analysis": analysis,
                "needs_thread": True
            }

        except Exception as e:
            self.logger.error(f"Failed to generate simple report: {str(e)}", exc_info=True)
            raise

    # ì„œë²„ ì •ë³´ CMDB (CSV) ì°¸ì¡°
    def _get_server_info(self, target: str) -> Dict:
        try:
            # CSV ìµœì‹  íŒŒì¼ ì„ íƒ
            files_config = self.config.get('files', {})
            csv_prefix = files_config.get('extdata_prefix')
            pattern = str(self.report.data_dir / f"{csv_prefix}*.csv")
            matching_files = glob.glob(pattern)
            
            if not matching_files:
                raise FileNotFoundError(f"CSV íŒŒì¼ íŒ¨í„´ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pattern}")
            
            latest_file = max(matching_files, key=lambda f: 
                int(''.join(filter(str.isdigit, os.path.basename(f))) or '0')
            )
            
            # Read CSV file
            df = pd.read_csv(latest_file, encoding='euc-kr')
            
            # Search by IP
            server_info = df[(df['ì‚¬ì„¤IP'] == target) | (df['ê³µì¸/NAT IP'] == target)]
            
            # Search by service name if IP not found
            if server_info.empty and target.startswith('service:'):
                service_name = target.split(':', 1)[1]
                server_info = df[df['ì„œë¹„ìŠ¤'] == service_name]
            
            if server_info.empty:
                raise ValueError(f"ë‹¤ìŒ ì„œë²„ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target}")
            
            return server_info.iloc[0].to_dict()
        
        except Exception as e:
            self.logger.error(f"Failed to get server info: {str(e)}")
            raise

    # Prometheus ë©”íŠ¸ë¦­
    async def _get_metrics(self, target: str, start_time: datetime, end_time: datetime) -> Dict[str, Any]:
        try:
            metrics = {}
            promql = self.config.get('prometheus', {}).get('promql', {})
            
            for metric_name, query in promql.items():
                try:
                    # Replace IP in query
                    formatted_query = query.replace('{ip}', target)
                    formatted_query = formatted_query.replace('{{', '{').replace('}}', '}')
                    
                    # Execute Prometheus query
                    result = await self.report.query_prometheus(formatted_query, start_time, end_time)
                    
                    if result and isinstance(result, list) and len(result) > 0:
                        if 'values' in result[0]:
                            values = [float(v[1]) for v in result[0]['values']]
                            metrics[metric_name] = {
                                'current': values[-1] if values else 0,
                                'average': float(pd.Series(values).mean()),
                                'maximum': float(pd.Series(values).max()),
                                'minimum': float(pd.Series(values).min()),
                                'values': values
                            }
                            continue
                    
                    self.logger.warning(f"No data returned for metric: {metric_name}")
                    metrics[metric_name] = self._empty_metric()
                
                except Exception as e:
                    self.logger.error(f"Failed to get metric {metric_name}: {str(e)}")
                    metrics[metric_name] = self._empty_metric()
            
            return metrics
        
        except Exception as e:
            self.logger.error(f"Failed to get metrics: {str(e)}")
            raise

    # ë¹ˆ ë©”íŠ¸ë¦­ ê°’ ì²˜ë¦¬
    def _empty_metric(self) -> Dict[str, Any]:
        return {
            'current': 0,
            'average': 0,
            'maximum': 0,
            'minimum': 0,
            'values': []
        }

    def _generate_header(self, server_info: Dict) -> str:
        """Generate report header section"""
        check_time = datetime.now().strftime('%Y-%m-%d %H:%M')
        return (
            f"ðŸ“‹ *ì„œë²„ ì ê²€ ë³´ê³ ì„œ*\n"
            f"ì ê²€ ì‹œê°„: {check_time}\n"
            f"ëŒ€ìƒ: {server_info['ITêµ¬ì„±ì •ë³´ëª…']} ({server_info['ì„œë¹„ìŠ¤']})"
        )

    # ì„œë²„ ì •ë³´ ì„¹ì…˜
    def _generate_basic_info(self, server_info: Dict) -> str:
        return (
            "ðŸ“Œ *ê¸°ë³¸ ì •ë³´*\n"
            f"â€¢ *ID:* {server_info['ID']}\n"
            f"â€¢ *Hostname:* {server_info['Hostname']}\n"
            f"â€¢ *IP:* {server_info['ì‚¬ì„¤IP']} / {server_info['ê³µì¸/NAT IP']}\n"
            f"â€¢ *OS:* {server_info['ì„œë²„ OS']} {server_info['ì„œë²„ OS Version']}\n"
            f"â€¢ *CPU:* {server_info['CPU Type']} ({server_info['CPU Core ìˆ˜']})\n"
            f"â€¢ *Memory:* {server_info['Memory']}\n"
            f"â€¢ *Disk:* {server_info['ë””ìŠ¤í¬ ìš©ëŸ‰']}"
        )

    # ë©”íŠ¸ë¦­ ì •ë³´ ì„¹ì…˜
    def _generate_metrics_info(self, metrics: Dict) -> str:
        """Generate metrics information section with visualizations"""
        try:
            sections = ["ðŸ“Š *ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ*"]
            viz_config = self.config.get('visualization', {})
            
            # CPU Metrics
            cpu_data = metrics.get('cpu_usage', self._empty_metric())
            cpu_gauge = self._create_gauge(cpu_data['current'], viz_config.get('slack_gauge', {}))
            cpu_trend = self._create_trend(cpu_data['values'], viz_config.get('slack_trend', {}))
            sections.append(
                f"â€¢ *CPU ì‚¬ìš©ë¥ :* {cpu_gauge} ({cpu_data['current']:.1f}%)\n"
                f"  â†³ í‰ê· : {cpu_data['average']:.1f}% / ìµœëŒ€: {cpu_data['maximum']:.1f}%\n"
                f"  â†³ ì¶”ì„¸: {cpu_trend}"
            )

            # Memory Metrics
            mem_data = metrics.get('memory_usage', self._empty_metric())
            mem_gauge = self._create_gauge(mem_data['current'], viz_config.get('slack_gauge', {}))
            mem_trend = self._create_trend(mem_data['values'], viz_config.get('slack_trend', {}))
            mem_total = metrics.get('memory_total', {}).get('current', 0) / (1024**3)  # Convert to GB
            mem_avail = metrics.get('memory_available', {}).get('current', 0) / (1024**3)
            sections.append(
                f"â€¢ *Memory ì‚¬ìš©ë¥ :* {mem_gauge} ({mem_data['current']:.1f}%)\n"
                f"  â†³ í‰ê· : {mem_data['average']:.1f}% / ìµœëŒ€: {mem_data['maximum']:.1f}%\n"
                f"  â†³ Total: {mem_total:.1f}GB / Available: {mem_avail:.1f}GB\n"
                f"  â†³ ì¶”ì„¸: {mem_trend}"
            )

            # Disk Metrics
            disk_data = metrics.get('disk_usage', self._empty_metric())
            disk_gauge = self._create_gauge(disk_data['current'], viz_config.get('slack_gauge', {}))
            disk_trend = self._create_trend(disk_data['values'], viz_config.get('slack_trend', {}))
            disk_read = metrics.get('disk_read_bytes', {}).get('current', 0) / (1024**2)  # MB/s
            disk_write = metrics.get('disk_write_bytes', {}).get('current', 0) / (1024**2)
            sections.append(
                f"â€¢ *Disk ì‚¬ìš©ë¥ :* {disk_gauge} ({disk_data['current']:.1f}%)\n"
                f"  â†³ í‰ê· : {disk_data['average']:.1f}% / ìµœëŒ€: {disk_data['maximum']:.1f}%\n"
                f"  â†³ I/O: Read {disk_read:.1f}MB/s / Write {disk_write:.1f}MB/s\n"
                f"  â†³ ì¶”ì„¸: {disk_trend}"
            )

            # Network Metrics
            net_rx = metrics.get('network_receive', {}).get('current', 0) / (1024**2)  # MB/s
            net_tx = metrics.get('network_transmit', {}).get('current', 0) / (1024**2)
            sections.append(
                f"â€¢ *Network íŠ¸ëž˜í”½:*\n"
                f"  â†³ Receive: {net_rx:.1f}MB/s\n"
                f"  â†³ Transmit: {net_tx:.1f}MB/s"
            )

            return "\n\n".join(sections)

        except Exception as e:
            self.logger.error(f"Failed to generate metrics info: {str(e)}")
            return "*ì‹œìŠ¤í…œ ì„±ëŠ¥ ì§€í‘œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ*"

    # ë¶„ì„ í¬ë©§ - LLM í”¼ë“œë°±
    async def _generate_analysis(self, server_info: Dict, metrics: Dict) -> str:
        try:
            # Create context for LLM
            context = {
                "ì„œë²„ì •ë³´": {
                    "í˜¸ìŠ¤íŠ¸ëª…": server_info['Hostname'],
                    "IP": server_info['ì‚¬ì„¤IP'],
                    "ì„œë¹„ìŠ¤": server_info['ì„œë¹„ìŠ¤'],
                    "ìš©ë„": server_info['ITêµ¬ì„±ì •ë³´ëª…'],
                    "ìš´ì˜ìƒíƒœ": server_info['ìš´ì˜ìƒíƒœ']
                },
                "ì‹œìŠ¤í…œì‚¬ì–‘": {
                    "OS": f"{server_info['ì„œë²„ OS']} {server_info['ì„œë²„ OS Version']}",
                    "CPU": f"{server_info['CPU Type']} ({server_info['CPU Core ìˆ˜']})",
                    "ë©”ëª¨ë¦¬": server_info['Memory'],
                    "ë””ìŠ¤í¬": server_info['ë””ìŠ¤í¬ ìš©ëŸ‰']
                },
                "ì„±ëŠ¥ì§€í‘œ": {
                    name: {
                        "í˜„ìž¬ê°’": f"{data['current']:.1f}%",
                        "í‰ê· ": f"{data['average']:.1f}%",
                        "ìµœëŒ€": f"{data['maximum']:.1f}%"
                    }
                    for name, data in metrics.items()
                    if isinstance(data, dict) and 'current' in data
                }
            }

            # Request LLM analysis
            ollama_config = self.config.get_config('ollama')
            prompt_config = self.config.get_config('prompt')

            request_data = {
                "model": ollama_config.get('model'),
                "prompt": f"{prompt_config.get('simple_analysis')}\n\nì‹œìŠ¤í…œ ì •ë³´:\n{context}",
                "stream": False
            }

            try:
                response = requests.post(
                    ollama_config.get('url'),
                    json=request_data,
                    timeout=float(ollama_config.get('timeout', 300))
                )
                
                if response.status_code == 200:
                    analysis = response.json()['response']
                    return analysis
                else:
                    return "ì‹œìŠ¤í…œ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            except requests.exceptions.RequestException as e:
                self.logger.error(f"LLM request failed: {str(e)}")
                return "LLM ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

        except Exception as e:
            self.logger.error(f"Failed to generate analysis: {str(e)}")
            return "ì‹œìŠ¤í…œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

    def _create_gauge(self, value: float, config: Dict) -> str:
        """Create visual gauge bar for Slack"""
        width = config.get('width', 10)
        chars = config.get('chars', {})
        filled_char = chars.get('filled', 'â– ')
        empty_char = chars.get('empty', 'â–¡')
        prefix = config.get('prefix', '')
        suffix = config.get('suffix', '')
        
        filled = int((value / 100) * width)
        return f"{prefix}{filled_char * filled}{empty_char * (width - filled)}{suffix}"

    def _create_trend(self, values: List[float], config: Dict) -> str:
        """Create trend visualization for Slack"""
        if not values:
            return ""
            
        width = config.get('width', 8)
        chars = config.get('chars', 'â–â–‚â–ƒâ–„â–…â–†â–‡â–ˆ')
        indicators = config.get('indicators', {})
        
        # Calculate trend direction
        if len(values) >= 2:
            start_avg = sum(values[:3]) / min(3, len(values))
            end_avg = sum(values[-3:]) / min(3, len(values))
            diff = end_avg - start_avg
            
            if abs(diff) < 5:  # threshold for "flat" trend
                indicator = indicators.get('flat', 'âž¡ï¸Ž')
            elif diff > 0:
                indicator = indicators.get('up', 'â¬†ï¸Ž')
            else:
                indicator = indicators.get('down', 'â¬‡ï¸Ž')
        else:
            indicator = indicators.get('flat', 'âž¡ï¸Ž')
        
        # Create sparkline visualization
        if len(values) < width:
            values_subset = values
        else:
            values_subset = []
            chunk_size = len(values) / width
            for i in range(width):
                start_idx = int(i * chunk_size)
                end_idx = int((i + 1) * chunk_size)
                chunk_avg = sum(values[start_idx:end_idx]) / (end_idx - start_idx)
                values_subset.append(chunk_avg)
        
        # Normalize values to char range
        min_val = min(values_subset)
        max_val = max(values_subset)
        if max_val == min_val:
            normalized = [0] * len(values_subset)
        else:
            normalized = [
                ((v - min_val) / (max_val - min_val)) * (len(chars) - 1)
                for v in values_subset
            ]
        
        # Create visualization
        trend = ''.join(chars[int(v)] for v in normalized)
        return f"{trend} {indicator}"

    def _format_size(self, size_bytes: float) -> str:
        """Format byte sizes for human reading"""
        units = ['B', 'KB', 'MB', 'GB', 'TB']
        unit_index = 0
        while size_bytes >= 1024 and unit_index < len(units) - 1:
            size_bytes /= 1024
            unit_index += 1
        return f"{size_bytes:.1f}{units[unit_index]}"